{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73db722c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-27 09:35:22,517 - kedro.io.data_catalog - INFO - Loading data from `parameters` (MemoryDataSet)...\n",
      "2022-03-27 09:35:22,517 - kedro.io.data_catalog - INFO - Loading data from `data_proc` (CSVDataSet)...\n",
      "2022-03-27 09:35:22,685 - kedro.io.data_catalog - INFO - Loading data from `vocabulary` (CSVDataSet)...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir(\"/Users/kipnisal/DS/BiblicalScripts/bib-scripts/\")\n",
    "sys.path.append(\"/Users/kipnisal/DS/BiblicalScripts/bib-scripts/src\")\n",
    "\n",
    "params = catalog.load('parameters')\n",
    "\n",
    "data = catalog.load(\"data_proc\")\n",
    "vocab = catalog.load(\"vocabulary\")\n",
    "\n",
    "known_authors = params['known_authors']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2d8632",
   "metadata": {},
   "source": [
    "## Issues: \n",
    "- with $c=2$, expect to get the same result either with one_vs_many or div_persuit. This is related to the fact that in 'diversity_persuit' the P-values are not uniform under the null -- check this!!\n",
    "- Looks like this is becasue we evaluate $SS_{fit}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b32a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f as fdist\n",
    "from scipy.stats import t as tdist\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import ttest_ind as ttest\n",
    "            \n",
    "from twosample import bin_allocation_test\n",
    "from multitest import MultiTest\n",
    "from typing import List\n",
    "\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8b6295fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CentroidSimilarity(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        self._global_mean = np.mean(X, 0)\n",
    "        self._global_std = np.std(X, 0)\n",
    "        \n",
    "        self._cls_mean = np.zeros((len(self.classes_),X.shape[1]))\n",
    "        self._cls_std = np.zeros((len(self.classes_),X.shape[1]))\n",
    "        self._cls_n = np.zeros((len(self.classes_),X.shape[1]))\n",
    "        \n",
    "        for i,_ in enumerate(self.classes_):\n",
    "            X_cls = X[y == self.classes_[i]]\n",
    "            self._cls_mean[i] = np.mean(X_cls, 0)\n",
    "            self._cls_std[i] = np.std(X_cls, 0)\n",
    "            self._cls_n[i] = len(X_cls)\n",
    "        self._mat = (self._cls_mean.T / np.linalg.norm(self._cls_mean, axis=1)).T\n",
    "        self._mask = np.ones_like(self._mat)\n",
    "\n",
    "        \n",
    "    def prob_func(self, response):\n",
    "        return np.exp(response) / (1 + np.exp(response))\n",
    "    \n",
    "    def get_centroids(self):\n",
    "        return self._mat * self._mask\n",
    "        \n",
    "    def predict_log_proba(self, X):\n",
    "        response = X @ self.get_centroids().T\n",
    "        return self.prob_func(response)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probs = self.predict_log_proba(X)\n",
    "        return np.argmax(probs, 1)\n",
    "\n",
    "class CentroidSimilarityFeatureSelection(CentroidSimilarity):\n",
    "    \n",
    "    def fit(self, X, y, method='one_vs_all'):\n",
    "        \n",
    "        super().fit(X, y)\n",
    "        self._cls_response = np.zeros(len(self.classes_))\n",
    "        \n",
    "        for i, cls in enumerate(self.classes_):\n",
    "            self._mask[i] = self.get_cls_mask(i, method=method)\n",
    "        \n",
    "    \n",
    "    def get_pvals(self, cls_id, method='one_vs_all'):\n",
    "        \"\"\"\n",
    "        compute P-values associated with each feature\n",
    "        for the given class\n",
    "        \"\"\"\n",
    "        \n",
    "        mu1 = self._cls_mean[cls_id]\n",
    "        n1 = self._cls_n[cls_id]\n",
    "        std1 = self._cls_std[cls_id]\n",
    "        nG = self._cls_n.sum(0)\n",
    "        stdG = self._global_std\n",
    "        muG = self._global_mean\n",
    "\n",
    "        assert(method in ['one_vs_all', 'diversity_persuit'])\n",
    "        if method == 'one_vs_all' :\n",
    "            pvals,_,_ = one_vs_all_ANOVA(n1, nG, mu1, muG, std1, stdG)\n",
    "        if method == 'diversity_persuit':\n",
    "            pvals,_,_ = diversity_persuit_ANOVA(self._cls_n,\n",
    "                                                self._cls_mean,\n",
    "                                                self._cls_std)\n",
    "        return pvals\n",
    "\n",
    "    \n",
    "    def get_cls_mask(self, cls_id, method='one_vs_all'):\n",
    "        \"\"\"\n",
    "        compute class feature mask\n",
    "        \"\"\"        \n",
    "        pvals = self.get_pvals(cls_id, method=method)\n",
    "        \n",
    "        mt = MultiTest(pvals)\n",
    "        hc, hct = mt.hc_star(gamma=.2)\n",
    "        self._cls_response[cls_id] = hc\n",
    "        mask = pvals < hct\n",
    "        \n",
    "        return mask\n",
    "        \n",
    "def diversity_persuit_ANOVA(nn, mm, ss):\n",
    "    \"\"\"\n",
    "    F-test for discoverying discriminating features\n",
    "    \n",
    "    The test is vectorized along the last dimention where\n",
    "    different entires corresponds to different features\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    :nn:  vector indicating the number of elements in each class\n",
    "    :mm:  matrix of means; the (i,j) entry is the mean response of\n",
    "          class i in feature j\n",
    "    :ss:  matrix of standard errors; the (i,j) entry is the standard\n",
    "          error of class i in feature j\n",
    "    \n",
    "    \"\"\"\n",
    "    muG = np.sum(mm * nn, 0) / np.sum(nn, 0) # global mean\n",
    "    SSfit = np.sum(nn * (mm - muG) ** 2, 0)  \n",
    "    SStot = np.sum(nn * (ss ** 2), 0)\n",
    "    SSerr = np.sum(nn * (ss ** 2), 0)\n",
    "    #SSerr = SStot - SSfit\n",
    "\n",
    "    dfn = len(nn) - 1\n",
    "    dfd = np.sum(nn, 0) - len(nn)\n",
    "\n",
    "    F = ( SSfit / dfn ) / ( SSerr / dfd )\n",
    "    return fdist.sf(F, dfn, dfd), SSerr, SSfit        \n",
    "        \n",
    "        \n",
    "def one_vs_all_ANOVA(n1, nG, mu1, muG, std1, stdG):\n",
    "    n2 = nG - n1\n",
    "    mu2 = (muG * nG - mu1 * n1) / (nG - n1)\n",
    "    SSfit = n1 * (mu1 - muG) ** 2 + n2 * (mu2 - muG) ** 2\n",
    "    #SSerr = std1 * n1 + std2 * n2 we don't know std2\n",
    "    SStot = stdG ** 2 * nG\n",
    "    SSerr = SStot - SSfit\n",
    "\n",
    "    F = ( SSfit / 1 ) / ( SSerr / (nG - 2) )\n",
    "    return fdist.sf(F, 1, nG - 2), SSerr, SSfit        \n",
    "        \n",
    "def two_sample_t_test(mean1, mean2, std1, std2, n1, n2):\n",
    "    assert n1 + n2 > 2\n",
    "    sp = np.sqrt( (std1 * (n1 - 1) + std2 * (n2 - 1)) / (n1 + n2 - 2)) \n",
    "    t = (mean1 - mean2) / sp\n",
    "    return tdist.sf(np.abs(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a2687b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(clf, X, y):\n",
    "    y_pred = clf.predict(X)\n",
    "    return np.mean(y_pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "e8da3da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atomic_exp(p, n, c, sig, mu, eps, test_frac):\n",
    "    \n",
    "    Z = np.random.randn(n, p)\n",
    "    y = np.random.randint(c, size=n)\n",
    "\n",
    "    mus = np.zeros((c, p))\n",
    "    for i in range(c):\n",
    "        idcs = np.random.rand(p) < eps\n",
    "        mus[i][idcs] = mu * (1-2*(np.random.rand(np.sum(idcs)) > .5)) / 2\n",
    "\n",
    "    true_mask = mus != 0\n",
    "\n",
    "    X = mus[y] + sig * Z\n",
    "\n",
    "    train_split_mask = np.random.rand(len(X)) > test_frac\n",
    "\n",
    "    X_train = X[train_split_mask]\n",
    "    y_train = y[train_split_mask]\n",
    "\n",
    "    X_test = X[~train_split_mask]\n",
    "    y_test = y[~train_split_mask]\n",
    "\n",
    "    dist_mat = pairwise_distances(mus)\n",
    "    delta_mean = np.sum(dist_mat) / (c * (c - 1))\n",
    "    delta_min = np.min(dist_mat + 1e9 * np.eye(len(dist_mat)))\n",
    "    \n",
    "    cs = CentroidSimilarity()\n",
    "    cs.fit(X_train, y_train)\n",
    "    acc = eval_accuracy(cs, X_test, y_test)\n",
    "\n",
    "    csf1 = CentroidSimilarityFeatureSelection()\n",
    "    csf1.fit(X_train, y_train, method='one_vs_all')\n",
    "    pvals1 = csf1.get_pvals(cls_id = 0, method='one_vs_all')\n",
    "    acc1 = eval_accuracy(csf1, X_test, y_test)\n",
    "\n",
    "    csf2 = CentroidSimilarityFeatureSelection()\n",
    "    csf2.fit(X_train, y_train, method='diversity_persuit')\n",
    "    pvals2 = csf2.get_pvals(cls_id = 0, method='diversity_persuit')\n",
    "    acc2 = eval_accuracy(csf2, X_test, y_test)\n",
    "\n",
    "    def get_FDR(csf, true_mask):\n",
    "        FP = np.sum(csf._mask.any(0)[~true_mask.any(0)])\n",
    "        TP = np.sum(csf._mask.any(0)[true_mask.any(0)])\n",
    "        return FP / (FP + TP)\n",
    "\n",
    "    hc1 = MultiTest(pvals1).hc()[0]\n",
    "    hc2 = MultiTest(pvals2).hc()[0]\n",
    "\n",
    "    return dict({'acc': acc, \n",
    "           'acc1': acc1, 'acc2': acc2,\n",
    "            'hc1': hc1, 'hc2': hc2,\n",
    "            'fdr1' : get_FDR(csf1, true_mask), \n",
    "            'fdr2' : get_FDR(csf2, true_mask),\n",
    "            'eps': eps,\n",
    "            'n' : n, 'p' : p, 'mu': mu, 'c': c, \n",
    "            'delta_mean': delta_mean,\n",
    "            'delta_min': delta_min,\n",
    "            'mask_sum' : np.sum(true_mask),\n",
    "           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "c1b69548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = .6\n",
    "((1 - np.sqrt(1-beta)) ** 2) * (beta >= .75) + (beta/2) * (beta < .75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "0a1e65e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "acc               0.221331\n",
       "acc1              0.272958\n",
       "acc2              0.273287\n",
       "c                 5.000000\n",
       "delta_mean        3.378386\n",
       "delta_min         3.082302\n",
       "eps               0.000514\n",
       "fdr1              0.985969\n",
       "fdr2              0.896540\n",
       "hc1               2.035384\n",
       "hc2               2.795632\n",
       "itr               4.500000\n",
       "mask_sum        132.700000\n",
       "mu                0.930367\n",
       "n               234.000000\n",
       "p             50000.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 50000\n",
    "n = int(2 * np.log(p) ** 2)\n",
    "c = 5\n",
    "sig = 1\n",
    "r = 0.4 / c\n",
    "beta = .7\n",
    "\n",
    "test_frac = .2\n",
    "mu = np.sqrt(r*np.log(p))\n",
    "eps = p ** (-beta)\n",
    "\n",
    "nMonte = 10\n",
    "df = pd.DataFrame()\n",
    "for itr in tqdm(range(nMonte)):\n",
    "    r = atomic_exp(p, n, c, sig, mu, eps, test_frac)\n",
    "    r['itr'] = itr\n",
    "    df = df.append(r, ignore_index=True)\n",
    "    \n",
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "1aef2e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.375,\n",
       " 'acc1': 0.625,\n",
       " 'acc2': 0.625,\n",
       " 'hc1': 1.7139266906616784,\n",
       " 'hc2': 1.7139266906615778,\n",
       " 'fdr1': 0.972027972027972,\n",
       " 'fdr2': 0.972027972027972,\n",
       " 'eps': 9.999999999999995e-05,\n",
       " 'n': 46,\n",
       " 'p': 100000,\n",
       " 'mu': 2.628260884878466,\n",
       " 'c': 2,\n",
       " 'delta_mean': 4.917025877137264,\n",
       " 'delta_min': 4.917025877137264,\n",
       " 'mask_sum': 14}"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = np.random.randn(n, p)\n",
    "y = np.random.randint(c, size=n)\n",
    "\n",
    "mus = np.zeros((c, p))\n",
    "for i in range(c):\n",
    "    idcs = np.random.rand(p) < eps\n",
    "    mus[i][idcs] = mu * (1-2*(np.random.rand(np.sum(idcs)) > .5)) / 2\n",
    "\n",
    "true_mask = mus != 0\n",
    "\n",
    "X = mus[y] + sig * Z\n",
    "\n",
    "train_split_mask = np.random.rand(len(X)) > test_frac\n",
    "\n",
    "X_train = X[train_split_mask]\n",
    "y_train = y[train_split_mask]\n",
    "\n",
    "X_test = X[~train_split_mask]\n",
    "y_test = y[~train_split_mask]\n",
    "\n",
    "dist_mat = pairwise_distances(mus)\n",
    "delta_mean = np.sum(dist_mat) / (c * (c - 1))\n",
    "delta_min = np.min(dist_mat + 1e9 * np.eye(len(dist_mat)))\n",
    "\n",
    "cs = CentroidSimilarity()\n",
    "cs.fit(X_train, y_train)\n",
    "acc = eval_accuracy(cs, X_test, y_test)\n",
    "\n",
    "csf1 = CentroidSimilarityFeatureSelection()\n",
    "csf1.fit(X_train, y_train, method='one_vs_all')\n",
    "pvals1 = csf1.get_pvals(cls_id = 0, method='one_vs_all')\n",
    "acc1 = eval_accuracy(csf1, X_test, y_test)\n",
    "\n",
    "csf2 = CentroidSimilarityFeatureSelection()\n",
    "csf2.fit(X_train, y_train, method='diversity_persuit')\n",
    "pvals2 = csf2.get_pvals(cls_id = 0, method='diversity_persuit')\n",
    "acc2 = eval_accuracy(csf2, X_test, y_test)\n",
    "\n",
    "def get_FDR(csf, true_mask):\n",
    "    #FP = np.sum(csf._mask > true_mask)\n",
    "    #TP = np.sum(csf._mask*true_mask)\n",
    "    FP = np.sum(csf._mask.any(0)[~true_mask.any(0)])\n",
    "    TP = np.sum(csf._mask.any(0)[true_mask.any(0)])\n",
    "    \n",
    "    return FP / (FP + TP)\n",
    "\n",
    "hc1 = MultiTest(pvals1).hc()[0]\n",
    "hc2 = MultiTest(pvals2).hc()[0]\n",
    "\n",
    "dict({'acc': acc, \n",
    "       'acc1': acc1, 'acc2': acc2,\n",
    "        'hc1': hc1, 'hc2': hc2,\n",
    "        'fdr1' : get_FDR(csf1, true_mask), \n",
    "        'fdr2' : get_FDR(csf2, true_mask),\n",
    "        'eps': eps,\n",
    "        'n' : n, 'p' : p, 'mu': mu, 'c': c, \n",
    "        'delta_mean': delta_mean,\n",
    "        'delta_min': delta_min,\n",
    "        'mask_sum' : np.sum(true_mask),\n",
    "       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "8538c4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.972027972027972"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FP = np.sum(csf1._mask.any(0)[~true_mask.any(0)])\n",
    "TP = np.sum(csf1._mask.any(0)[true_mask.any(0)])\n",
    "FP / (FP + TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "a134daec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csf1._mask.any(0)[true_mask.any(0)].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17649188",
   "metadata": {},
   "source": [
    "## Bible Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "183ddca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biblical_scripts.pipelines.sim.nodes import (\n",
    "    build_model, model_predict, _prepare_data)\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "cfcd7765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 11:18:18,238 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "data = _prepare_data(data[data.to_report])\n",
    "data_known = data[data.author.isin(known_authors)]\n",
    "lo_docs = data_known.doc_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "5d39c838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/49 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 11:18:18,376 - root - INFO - Building CompareDocs model using 48 documents. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/49 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'OneVsMany' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-426-646c1d0b82eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_known\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# leave-one-out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneVsMany\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_MultiDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_class_means\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OneVsMany' is not defined"
     ]
    }
   ],
   "source": [
    "df_res = pd.DataFrame()\n",
    "for doc in tqdm(lo_docs): # leave-one-out\n",
    "    ds1 = data_known[data_known.doc_id == doc]\n",
    "    data_train = data_known.drop(ds1.index) # leave-one-out\n",
    "    md, vocab = build_model(data_train, vocab, params['model'])\n",
    "    om = OneVsMany()\n",
    "    om.fit_MultiDoc(md)\n",
    "    om.fit_class_means(md, data_train)\n",
    "    r = om.predict_proba(ds1)\n",
    "    #r = om.predict_response(ds1)\n",
    "    res = pd.DataFrame(r, index = ['avg']).T.reset_index().rename(columns={'index' : 'cls'})\n",
    "    res['doc_tested'] = doc\n",
    "    res['len'] = len(ds1)\n",
    "    df_res = df_res.append(res, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d0fa000b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with pval_2smp = 0.5714285714285714\n",
      "Accuracy with pval_bartlett = 0.5306122448979592\n",
      "Accuracy with pval_f = 0.6326530612244898\n",
      "Accuracy with pval_levene = 0.3673469387755102\n",
      "Accuracy with pval_t = 0.8367346938775511\n",
      "Accuracy with pval_tLDA = 0.8571428571428571\n",
      "Accuracy with pval_wilcox = 0.4897959183673469\n"
     ]
    }
   ],
   "source": [
    "df_res.loc[: ,'true_class'] = df_res.doc_tested.apply(lambda x : x.split('|')[0])\n",
    "#df_res.loc[:, 'neg_raw'] = -df_res['raw']\n",
    "\n",
    "for value in df_res.columns[df_res.columns.str.contains(r'pval|neg_raw|avg')]:        \n",
    "    idx = df_res.groupby('doc_tested')[value].idxmax(axis=1)\n",
    "    acc = np.mean(df_res.loc[idx, 'cls'] == df_res.loc[idx, 'true_class'])\n",
    "    print(f\"Accuracy with {value} = {acc}\")\n",
    "    #df_res['predicted_class'] = df_res[['Dtr', 'DtrH', 'P']].idxmin(axis=1)\n",
    "    #print(\"Accuracy: \", np.mean(df_res.predicted_class == df_res.true_class))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BiblicalScripts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
