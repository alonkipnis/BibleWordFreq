# -*- coding: utf-8 -*-
"""BiblicalScripts_morphological_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GOJ6KWV23fxXQCB4cqq4NWtpMb5PAjhR

# Authorship in the Bible
Authorship of various biblical text. 

Updates 5/22/2020:
- Using ng_range=(2,2)  and 200 word per author brings good results for HC and HC_rank
- Under these parameters, P-value from simulated null leads to the same results as HC_rank, hence, in some sense it valuadates the use of the HC_rank 
- To do: 
  - Evaluate simulated null with more points (say 1,000)
  - To get better rank accuracy, we can merge we can merge several (say two) chapters of known authorship

To do: 
 - Merge knonwn doc to get larger docs for null distribution.

## Bible with morphology info
OpenBible project inforation: <a href = 'https://hb.openscriptures.org/index.html'>https://hb.openscriptures.org/index.html</a> <br>
Data is avaialbe at <a href='https://github.com/openscriptures/morphhb/tree/master/wlc'>https://github.com/openscriptures/morphhb/tree/master/wlc</a> <br>
Information on morphological code is available at <a href='https://hb.openscriptures.org/parsing/HebrewMorphologyCodes.html'>https://hb.openscriptures.org/parsing/HebrewMorphologyCodes.html</a>
"""

#!git clone https://github.com/alonkipnis/AuthorshipAttribution.git
#!git clone https://github.com/alonkipnis/TwoSampleHC.git HC
#!mv ./HC/TwoSampleHC/ ./TwoSampleHC

import pandas as pd
import numpy as np
import re

#import auxiliary functions for python
import sys
sys.path.append('./AuthorshipAttribution')
sys.path.append('./TwoSampleHC')
sys.path.append('./morphhb-master')
from plotnine import *
from AuthAttLib import *
from visualize_HC_scores import *
from tqdm import tqdm

"""## Load and Arrange Data

### Read raw data

#### Download:
"""

# get bible with morphological info
#!git clone https://github.com/openscriptures/morphhb.git

"""#### Reference Data
-Deut
  - Deut 6, 8-13, 15, 16, 18, 19, 26, 27, 28;

- Dtr History
  - 2 Sam 7, Jos 1, 5, 6, 12, 23; Judg 2, 6; 1 Kgs 8; 2 Kgs 17, versets 1-21; 2 Kgs 22 – 25;

- P
  - Gen 17; Ex 6, 16, 25-31, 35-40; Leviticus 1-4 and 8-9

#### Unk:
- Test case 1.1: Ark narrative 1
  -  1 Sam 4-6  (joined)

- Test case 1.2: Ark narrative 2
  - 2 Sam 6

- Test case 2: Late Abraham material
  - Genesis 14, 15, 20, 22, 24

- Test case 3: The Gibea-story
  - Judges 19-21

- Test case 4: Early Jacob story
  - Gn 25,7
  - Gn 25,24-26
  - Gn 28,10-22
  - Gn 29,15-30
  - Gn 30,25-42
  - Gn 31,1-22
  - Gn 31,46-47

- Test case 5.1: Parts in Chronicles 1
  - 1Ch 12,8-15.23-40 **
  - 1Ch 22,1-29,21 

- Test case 5.2: parts in Chronicles 2
  - 2Ch 19,1-20,30
  - 2Ch 29,3-31,21
  - 2Ch 33,11-17
  - 2Ch 34,3-7
  - 2Ch 36,22-23

- Test case 6: Esther
  - Esther

- Test case 7: Proverbs wisdom literature
  - Pr 10-31



#### To do:
- Verify that we load the correct book/chapter/verse according to the list above
"""

# Chapters of known authorship

Dtr = {('Deut', 6), ('Deut', 8), ('Deut', 9), ('Deut', 10), ('Deut', 11),
       ('Deut', 12), ('Deut', 13), ('Deut', 15), ('Deut', 16), ('Deut', 18), ('Deut', 19),
       ('Deut', 26), ('Deut', 28), ('Deut', 27)} #Deut 6, 8-13, 15, 16, 18, 19, 26, 27, 28;

DtrH = {('2Sam', 7), 
        ('Josh', 1), ('Josh', 5), ('Josh', 6), ('Josh', 12), ('Josh', 23), ('Judg', 2),
        ('Judg', 6), 
        ('1Kgs', 8), ('2Kgs', 17, '1:21'), ('2Kgs', 22), ('2Kgs', 23), ('2Kgs', 24), ('2Kgs', 25)}

P = {('Gen', 17), ('Exod', 26), ('Exod', 27), ('Exod', 28),
    ('Exod', 16), ('Exod', 35), ('Exod', 36), ('Exod', 37), ('Exod', 38), ('Exod', 39),
    ('Exod', 40), ('Exod', 25), ('Exod', 29), ('Exod', 30), ('Exod', 31), ('Exod', 6),
    ('Lev', 1), ('Lev', 2), ('Lev', 3), ('Lev', 4), ('Lev', 8), ('Lev', 9)} 

ark_narrative1 = { ('1Sam', 4), ('1Sam', 5), ('1Sam', 6)}

ark_narrative2 = { ('2Sam', 6)}

Late_Abraham_material_in_Genesis = {('Gen', 14), ('Gen', 15), ('Gen', 20),
                           ('Gen', 22), ('Gen', 24)} #Genesis 14, 15, 20, 22, 24

Gibea_story_in_Judges = {('Judg', 19), ('Judg', 20), ('Judg', 21)} #Judges 19-21

Early_Jacob_story = {('Gen', 25,'24:25'), ('Gen', 28, '10:22'), ('Gen', 29, '15:30'),
                     ('Gen', 30, '25:42'), ('Gen', 31, '1:22'), ('Gen', 31, '46:47')}

Parts_Chronicles1 = {
    ('1Chr', 12, '8:15'), ('1Chr', 12, '23:40'), ('1Chr', 22), ('1Chr', 23), ('1Chr', 24),
    ('1Chr', 25), ('1Chr', 26), ('1Chr', 27), ('1Chr', 28), ('1Chr', 29, '1:21')}

Parts_Chronicles2 = { ('2Chr', 19), ('2Chr', 20, '1:30'), ('2Chr', 29, '3:50'),  ('2Chr', 30),
    ('2Chr', 31,'1:21'), ('2Chr', 33, '11:17'), ('2Chr', 34, '3:7'), ('2Chr', 36, '22:23')}

Esther = {('Esth', 1), ('Esth', 2), ('Esth', 3), ('Esth', 4), ('Esth', 5), ('Esth', 6)}

Proverbs = {('Prov', 10), ('Prov', 11), ('Prov', 12), ('Prov', 13), ('Prov', 14), ('Prov', 15),
            ('Prov', 16), ('Prov', 17), ('Prov', 18), ('Prov', 19), ('Prov', 20), ('Prov', 21),
            ('Prov', 22), ('Prov', 23), ('Prov', 24), ('Prov', 25), ('Prov', 26), ('Prov', 27),
            ('Prov', 28), ('Prov', 29), ('Prov', 30), ('Prov', 31)}

"""### Read data
After downloading bible with morphological info and reading into a dataframe, the script below saves the data into a file. After running once, can read directly from the file.
"""

# load lemmas from files at https://github.com/openscriptures/morphhb/tree/master/wlc
# save dataset in file BiblicalScript_data.csv, so only need to run once

from xml.dom.minidom import parse, parseString

def load_chapter(book, chapter, verse_set = []) :
    path = './morphhb/wlc/' + book + '.xml'
    df = pd.DataFrame()
    bookxml = parse(path)
    chapterlist = bookxml.getElementsByTagName('chapter')
    chapterlist = [ch for ch in chapterlist if ch.attributes['osisID'].value == book+'.'+str(chapter)]
    for chap in chapterlist:
        verselist = chap.getElementsByTagName('verse')
        for verse in verselist:
            mywelements = verse.getElementsByTagName('w')
            for el in mywelements:
                vrs = verse.attributes['osisID'].value
                vrs_numeric = int(vrs.split('.')[-1])
                if len(verse_set) == 0 or vrs_numeric in verse_set :
                    df = df.append({'lemma' : el.attributes['lemma'].value,
                                    'morph' : el.attributes['morph'].value,
                                    'term' : el.firstChild.data,
                                    'chapter' : chap.attributes['osisID'].value,
                                    'verse' : vrs
                                   }, ignore_index=True)
    return df


data = pd.DataFrame()
auth_dic = {'Dtr': Dtr, 'DtrH' : DtrH, 'P' : P,
            'UNK1.1:Ark1' : ark_narrative1, 
            'UNK1.2:Ark2' : ark_narrative2, 
            'UNK2:Abraham' : Late_Abraham_material_in_Genesis,
            'UNK3:Gibea' : Gibea_story_in_Judges,
            'UNK4:Jacob' : Early_Jacob_story,
            'UNK5.1:Chronicles1' : Parts_Chronicles1,
            'UNK5.2:Chronicles2' : Parts_Chronicles2,
            'UNK6:Esther' : Esther,
            'UNK7:Proverbs' : Proverbs
           }

i=0          
for auth in auth_dic.keys() :
    for d in auth_dic[auth] :
        try :  #check if verse range is specified
          st = d[2]
          l, r = st.split(':')
          verse_set = list(range(int(l),int(r)+1))
        except :
          verse_set = []
        df = load_chapter(d[0], d[1], verse_set=verse_set)
        df.loc[:,'author'] = auth
        
        if i>=3:
            df.loc[:,'chapter'] = auth
        
        data = data.append(df, ignore_index = True)
    i=i+1
 
    

        
data.to_csv('./drive/My Drive/Data/BiblicalScript_data.csv')
#df contains all lemmas from all chapters.

"""### Load data from Google Drive"""

#from google.colab import drive
#drive.mount('/content/drive')

data = pd.read_csv('./drive/My Drive/Data/BiblicalScript_data.csv')

"""### Remove specific chapters (only if needed)"""

# to_remove = ['Gen.17', 'Exod.6', 'Exod.16', 'Deut.9']
# data = data[~data.chapter.isin(to_remove)]

# change authorship
# data.loc[data.chapter.isin(['Exod.6', 'Exod.16', 'Gen.17']), 'author'] = 'DtrH'

"""### Merge UNK chapters"""

# merge Ark stories:
data.loc[data.author.str.contains('UNK1'), 'author'] = 'UNK1:Ark'

#merge
lo_known_authors = data.author[~data.author.str.contains('UNK')].unique().tolist()
lo_unk_authors = data.author[data.author.str.contains('UNK')].unique().tolist()
for auth in lo_unk_authors :
    data.loc[data.author == auth,'chapter'] = auth

import plotnine

plotnine.options.figure_size = (7.4, 14)

p = (ggplot(aes(x = 'chapter', fill='author'), data=data ) + geom_bar(stat='count')
+ ylab('# of lemmas') + coord_flip()
)
print(p)

"""### Pre-process data:
- remove proper names (or other parts of speech)
- count connecting words seperately (optional)
"""

def process_morphology(ds) :
    #remove proper names:
    
    def remove_POS(df, code) :
        return df[~df.morph.str.containt(code)]
    
    def replace_POS(df, code, value) :
        df.loc[df.morph.str.contains(code), 'lemma'] = value
        return df
    
    def keep_only_POS(df, code)  :
        return df[df.morph.str.contains(code)]
    
    ds = replace_POS(ds, 'Np', 'PROPN') #remove proper names
    #ds = replace_POS(ds, 'Nc', 'COMN') # remove common nouns
    #ds = replace_POS(ds, 'Ng', 'GENN') # remove gentilic nouns    
    #ds = replace_POS(ds, 'Ac', 'CD') # remove cardinal digits
    #ds = replace_POS(ds, 'V', 'VERB') # remove verbs
    
    return ds

dictionary = dict([(k,v) for k,v in zip(data.lemma.values,data.term.values)] )

def lemma_to_term(lemma) :
  try :
    return dictionary[lemma]
  except :
    return lemma

dictionary1 = dict([(k,v) for k,v in zip(data.lemma.values,data.morph.values)] )
def lemma_to_morph(lemma) :
  try :
    return dictionary1[lemma]
  except :
    return lemma

def ngram_to_term(ng) :
   return [lemma_to_term(l) for l in ng ]

ds = data.copy()
ds = process_morphology(ds)
ds = ds.filter(['chapter', 'lemma', 'author', 'morph', 'verse'])\
          .rename(columns = {'chapter' : 'doc_id', 'lemma' : 'term'})

"""#### remove_prefixes """

        
        
        
"""#### Optional: unnest connecting words"""

#optional: add prefix and suffix as additional terms
def explode_str(df, col, sep):
    # add new rows by splitting strings in 'col' according to 'sep'
    s = df[col]
    i = np.arange(len(s)).repeat(s.str.count(sep) + 1)
    return df.iloc[i].assign(**{col: sep.join(s).split(sep)})

remove_prefixes = False
print ('remove_prefixes')
if (remove_prefixes):
    for i in range(0, len(data)):
        current = ds['term'][i]
        #sp_current = current.split('/')
        #current = sp_current[-1]
        sp_current = current.split(' ')
        ds['term'][i] = sp_current[0]
        
        # current = ds['lemma'][i]
        # sp_current = current.split('/')
        # current = sp_current[-1]
        # sp_current = current.split(' ')
        # ds['lemma'][i] = sp_current[0]
        
 
ds = explode_str(ds, 'term', '/')
"""#### Optional: use ngrams"""

ng_range = (2,2)
ds = extract_ngrams(ds, ng_range = ng_range)	# split the term b6256 to	('b', '6256')

       


"""#### Most common words:"""

N_WORDS = 200

def n_most_frequent_by_author(ds, n) :
  terms = ds.groupby(['author','term'])\
            .count()\
            .sort_values('doc_id', ascending=False)\
            .reset_index()\
            .groupby(['author'])\
            .head(n)

  return terms.term.unique().tolist()

most_freq = n_most_frequent_by_author(ds[ds.author.isin(['Dtr', 'DtrH', 'P'])],
                                      N_WORDS)
print("size of vocabulary = ", len(most_freq))

"""## Build Model

- Assumes that 'data' is a Pandas DataFrame with columns: 'author','text'
- Model is associated with pair of authors. Need to make pairwise comparisons in order to attribute authorship with more than two candidate authors.
- For best results, set the model to ignore proper names by including them in 'words_to_ignore'.

#### Train model
"""

MIN_CNT = 10 #@param {type:"slider", min:3, max:30, step:1}
ALPHA = 0.3 #@param {type:"slider", min:0.05, max:0.5, step:0.05}

ds = ds.sort_values(by=['doc_id']) # the sort is important for the consistency with the bootstrap 
model = AuthorshipAttributionDTM(ds, 
                              min_cnt = MIN_CNT,
                              vocab = most_freq,
                              alpha=ALPHA)

"""## Analysis

#### Get discriminating words:
"""

doc1 = ('Dtr',None)
doc2 = ('DtrH',None)
df1 = model.two_doc_test(doc1, doc2)
df1 = df1.sort_values('pval')
df1.loc[:, 'feat_trans'] = df1.feat.apply(ngram_to_term)
df1[df1.thresh]

"""#### Compute leave-one-out cross validation results to asses model performance"""

#compute scores (each doc against each corpus)
df = model.internal_stats(LOO = True, wrt_authors=['Dtr', 'DtrH', 'P']) #use LOO = true only for rank-based testing
df = df.dropna()

"""#### Success probability using ground truth data (using leave one out)
Some of the documents are very short, so we can expect worse performance in validation with ground truth than in practice (no model fitting so no overfitting).
"""

def prob_of_succ(df, value, known_authors, plot=False) :
  # compute prob of success for each labeled doc:
  df.loc[:,value] = df[value].astype(float)
  res1 = df[df.author.isin(known_authors) & df.wrt_author.isin(known_authors)].reset_index()
  idx_min = res1.groupby(['doc_id', 'author'])[value].idxmin()
  res_min = res1.loc[idx_min, :]
  res_min.loc[:, 'succ'] = res_min.author == res_min.wrt_author

  succ_per_doc = res_min.groupby('doc_id').succ.mean()
  print("{} prob. of success = {}".format(value,succ_per_doc.mean()) )
  if plot :
    p = ggplot(aes(x='doc_id', fill = 'succ'), data = res_min) + geom_bar(stat='count') + coord_flip()
    print(p)

prob_of_succ(df, value = 'HC', known_authors=['Dtr', 'P', 'DtrH'])
prob_of_succ(df, value = 'HC_rank', known_authors=['Dtr', 'P', 'DtrH'])

prob_of_succ(df, value = 'chisq', known_authors=['Dtr', 'P', 'DtrH'])
prob_of_succ(df, value = 'chisq_rank', known_authors=['Dtr', 'P', 'DtrH'])

prob_of_succ(df, value = 'log-likelihood', known_authors=['Dtr', 'P', 'DtrH'])
prob_of_succ(df, value = 'log-likelihood_rank', known_authors=['Dtr', 'P', 'DtrH'])

"""#### Illustrate min-rank results"""

import plotnine
known_authors=['Dtr', 'P', 'DtrH']

value = 'HC'
plotnine.options.figure_size = (4, 5)

p = (ggplot(aes(x = 'doc_id', y = value, fill = 'wrt_author'),
data =df[df.wrt_author.isin(known_authors) & df.author.str.contains('UNK')]) 
+ geom_col(position = 'dodge2', size=5)
+ xlab('') + ylab(value) + coord_flip()
)

p.save('./drive/My Drive/Figs/res_UNK_{}_w{}.png'\
        .format(value,N_WORDS))
  
print(p)
df.to_csv(r"C:\Users\golovin\Google Drive\Phd\Articles\NLP_Bible\Code\Authorship\Authorship\results_python\HC_results.csv")


# res_min = res1.loc[idx_min, :]
# res_min.loc[:, 'succ'] = res_min.author == res_min.wrt_author

"""#### t-Test for association"""

plotnine.options.figure_size = (3, 5)
import matplotlib
from matplotlib import pyplot as plt
from scipy.stats import t


for wrt_author in ['P', 'Dtr', 'DtrH'] :
  df_test = df[(df.wrt_author == wrt_author) & (df.author.str.contains('UNK'))]
  df_null = df[(df.wrt_author == wrt_author) & (df.author == wrt_author)]

  mu = df_null.HC.mean()
  std = np.sqrt(df_null.HC.var() / (len(df_null) - 1 ))

  dist = scipy.stats.t(df= len(df_null), loc=mu,scale=2*std)

  def t_test(x, mu, std) :
    return dist.sf(x)

  df_test.loc[:,'-log(p)'] = df_test.HC.apply(lambda x : -np.log(t_test(x, mu, std)) )

  scipy.stats.probplot(df_null.HC, dist=dist, plot=matplotlib.pyplot)[1]
  plt.show()
  p.save('./drive/My Drive/Figs/res_ttest_plot_{}.png'.format(wrt_author))

  p =(ggplot(aes(x = 'doc_id', y = '-log(p)'), data = df_test ) + geom_col()
  + coord_flip() + ggtitle(wrt_author) + geom_hline(yintercept = -np.log(0.05), color='red', linetype='dashed')
  )

  p.save('./drive/My Drive/Figs/res_ttest_{}.png'.format(wrt_author))

  print(p)

"""#### Illustrate leave-one-out results"""

def plot_author_pair(df, value = 'HC', wrt_authors = [],
                     show_legend=True):
    
    df.loc[:,value] = df[value].astype(float)
    df1 = df.filter(['doc_id', 'author', 'wrt_author', value])\
            .pivot_table(index = ['doc_id','author'],
                         columns = 'wrt_author',
                         values = [value])[value].reset_index()

    lo_authors = pd.unique(df.wrt_author)
    no_authors = len(lo_authors)
    
    if no_authors < 2 :
        raise ValueError
    
    if wrt_authors == [] :
        wrt_authors = (lo_authors[0],lo_authors[1])

    color_map = LIST_OF_COLORS

    df1.loc[:, 'x'] = df1.loc[:, wrt_authors[0]].astype('float')
    df1.loc[:, 'y'] = df1.loc[:, wrt_authors[1]].astype('float')
    p = (
        ggplot(aes(x='x', y='y', color='author', shape = 'author'), data=df1) +
        geom_point(show_legend=show_legend, size = 3) + geom_abline(alpha=0.5) +
        # geom_text(aes(label = 'doc_id', check_overlap = True)) +
        xlab(wrt_authors[0]) + ylab(wrt_authors[1]) +
        scale_color_manual(values=color_map) +  #+ xlim(0,35) + ylim(0,35)
        theme(legend_title=element_blank(), legend_position='top'))
    return p

value = 'HC'

plotnine.options.figure_size = (7, 6)

for auth_pair in [['Dtr', 'P'],['DtrH', 'P'], ['Dtr', 'DtrH']] :
  df_disp = df
  df_disp = df[df.author.isin(auth_pair)]

  p = plot_author_pair(df_disp, value = value, wrt_authors=auth_pair)
  
  p.save('./drive/My Drive/Figs/res_known_{}_{}_vs_{}_w{}.png'\
        .format(value, auth_pair[0],auth_pair[1],N_WORDS))
  print(p)

"""## Bootstrapping

### Boostrap over lemmas
"""
from random import choices

run_boot_strap = True
############ 1. Bootstrap words ##########
if (run_boot_strap):
    
    # calculate how many terms exist. sample this number
    lo_term = pd.unique(ds.term)
    
    # randomly sample the words with replacment
    #indexces = np.sort(choices(range(0, len(lo_term)), k=len(lo_term)))
    #count for each word, the number of it repetitions
    num_boot_iter = 100
    statistics = []
    HC_stat = []
    
    #perform bootstrap iterations
    for b_i in range(0, num_boot_iter):
        print(b_i)
        k = len(lo_term) # take all the unique words and randmply sample k from them with replacment
        indexces = np.sort(choices(range(0, k), k=k))
    
        indexces_hist = pd.Series(indexces).value_counts().reset_index().values.tolist() # create a histogram of the words
        newsturct = []

        # for each term in the histogram, create a coresponding configuration of thers
        # replace repeated terms with "Term_2", where the last one is a running number
        for i in range(0,len(indexces_hist)):
            c_i = indexces_hist[i]
            
            #filter all that match c_i[0][0], and rename them to "Term_2",
            for t in range(0, c_i[1]):
                ds_auth = ds[ds.term == lo_term[c_i[0]]]
                ppp = pd.DataFrame(ds_auth.loc[:, 'term'].tolist())
                ppp1 = str(t) +  '_'  + ppp[1] 
                ds_auth.loc[:, 'term'] = list(zip(ppp[0], ppp1))
                
                #concatinate to the data structure
                if i==0:
                    newsturct =  ds_auth
                else:
                    newsturct = pd.concat([newsturct, ds_auth]) 
                    
                    
        
        # collect statistics: doc_id, wrt_author, HC - list
        newsturct = newsturct.sort_values(by=['doc_id'])  

        # create a model with the new words dictionary              
        model = AuthorshipAttributionDTM(newsturct, min_cnt = 5, randomize = False)
    
        #compute scores (each doc against each corpus)
        df = model.internal_stats(LOO = False) #use LOO = true only for rank-based testing
    
        d2 = np.asarray(df['HC'].tolist())
        d3 = d2.reshape(len(d2), 1)       
    
               
        if b_i==0:
            HC_stat = d3
        else:
            if len(d3)==len(HC_stat):
                HC_stat = np.concatenate((HC_stat, d3),axis=1)
                
        #hc_stat = run _hc(newsturct)
    'doc_id', 'wrt_author', 
    alpha = 0.8
    lower = []
    upper = []
    for i in range(0, len(HC_stat)):
        current_comparison = HC_stat[i,:];
        ordered = np.sort(current_comparison)
        lower.append(np.percentile(ordered, 100*(1-alpha)/2))
        upper.append(np.percentile(ordered, 100*(alpha+((1-alpha)/2))) )
    lower = np.asarray(lower).reshape(len(lower), 1)      
    upper = np.asarray(upper).reshape(len(upper), 1)      
    
    c = np.concatenate((lower, upper),axis=1)
    np.savetxt(r"C:\Users\golovin\Google Drive\Phd\Articles\NLP_Bible\Code\Authorship\Authorship\pythonCodeV2\drive\My Drive\Data\HC_lower_uperbounts.csv", c, delimiter=",")
    df.to_csv(r"C:\Users\golovin\Google Drive\Phd\Articles\NLP_Bible\Code\Authorship\Authorship\results_python\HC_results_boot_stap.csv")
    
    np.savetxt(r"C:\Users\golovin\Google Drive\Phd\Articles\NLP_Bible\Code\Authorship\Authorship\pythonCodeV2\drive\My Drive\Data\HC_stat_100.csv", HC_stat, delimiter=",")


############ Bagging- Accuracy evaluation ##########
acc_sum = 0
acc = []
corpus_size = [14,14,22]  #8,8,8)#
max_n_p = 0.75
max_n_prim_p = 0.8
max_n =  [round(i * max_n_p) for i in corpus_size]
max_n_prim = [round(i *max_n_prim_p ) for i in max_n]
lo_corp = [1, 2, 3]
lo_corp_names = ['Dtr', 'DtrH', 'P']
t_p = 1
  
import itertools
import numpy
accuracy_all = list();

# perform bagging several times
# each time create a random train test corpora, with proportion 0.75, 0.25
#then sample the test 0.8, 0.2, and calculate accuracy score

for ttt in range(1,10):   
    cor_all = []

    train_inx_ids = list()
    test_inx_ids = list()
    # for each of the 3 corpora, sample equal proportional number of texts
    for c in lo_corp:
      i = c-1
      # select the current corpus
      current_corpus = ds[ds.author==lo_corp_names[i]]
      chapters = pd.unique(current_corpus.doc_id)
      
      rand_inx = numpy.random.permutation(corpus_size[i])
      
      train_inx = rand_inx[0:max_n[i]]
      test_inx  = rand_inx[max_n[i]+1:corpus_size[i]]
      test_inx  = test_inx[0:(corpus_size[i]-max_n[i])]
      
      train_inx_id = chapters[train_inx]
      test_inx_id = chapters[test_inx]
      
      if i==0:
          train_inx_ids = list(train_inx_id)
          test_inx_ids =  list(test_inx_id)
      else:
          train_inx_ids = list(itertools.chain(train_inx_ids, list(train_inx_id)))
          test_inx_ids = list(itertools.chain(test_inx_ids, list(test_inx_id))) 


    
    print(train_inx_ids)
    print('**************')
    print(test_inx_ids)
  
    
   
    ######## Phase 2, sample again and run the model
    
    #sample of each writer, with replacment
    
    iteration_num = 10
    t_ids = test_inx_ids
      
    for i in range(0, iteration_num):
      accuracy=list()
      exact = 0
      q = 1
      print(i)
      
      for t in range(0,len(t_ids)): #length(t_ids)
        d = t_ids[t]
        #print(d)
        train_inx_ids_2 = list()
        j = 0
        score_wrt_corpus = []
        
        for k in lo_corp:
          c = k-1
      
          train_current_doc_names = train_inx_ids[j:(j+max_n[c])]
    
          j = j+max_n[c]
          
          train_inx_2 = numpy.random.permutation(max_n[c])
          train_inx_2 = train_inx_2[0:max_n_prim[c]].astype(int)
          
          
          train_inx_id_2 = [train_current_doc_names[k] for k in train_inx_2] 
          
          train_inx_ids_2 = list(itertools.chain(train_inx_ids_2, list(train_inx_id_2))) 
          
             
              
        filtered_data = list();
                
        ids = train_inx_ids_2
        for hh in range(0, len(ids)):
            print( len(ds[ds.doc_id==ids[hh]]))
            if hh==0:
                filtered_data = ds[ds.doc_id==ids[hh]]
            else:
                filtered_data = filtered_data.append(ds[ds.doc_id==ids[hh]])
        filtered_data = filtered_data.append(ds[ds.doc_id==d])
               
        #run the model          
        newsturct = filtered_data.sort_values(by=['doc_id'])                
        model = AuthorshipAttributionDTM(newsturct, min_cnt = 5, randomize = False)
      
        #compute scores (each doc against each corpus)
        df = model.internal_stats(LOO = False) #use LOO = true only for rank-based testing
        tested_doc = df[df.doc_id==d]
        min_index = np.argmin(tested_doc.HC)
        authers = tested_doc.wrt_author
        predicted_corpus  = authers.values[min_index]
    
    
        expected_author = tested_doc.author
        expected_author  = expected_author.values[0]
        accuracy.append( predicted_corpus==expected_author)
    
      accuracy_all.append(sum(accuracy)/len(accuracy))

     
print(accuracy_all)
print(np.median(accuracy_all))



    
"""### Bootstrap 2 
To do: 
- attribute unknown documents and find bootstrapped probability of association of these documents. 
- Have an exact binomial test for the significance of each association (number of success should be significantly greater than 1/3)
"""

seed = 100
nBS = 100 #@param {type:"slider", min:0, max:1000, step:100}

for vocab_size in [500] :
  most_freq = n_most_frequent_by_author(
      ds[ds.author.isin(['Dtr', 'DtrH', 'P'])],
                                      vocab_size)

  res = pd.DataFrame()
  for i in range(nBS) :
    # optional: one bootstrap iteration:
    dsBS = ds.sample(len(ds), replace=True, random_state=seed+i)

    model = AuthorshipAttributionDTM(dsBS, 
                                    vocab = most_freq,
                                    min_cnt = MIN_CNT,
                                        alpha = ALPHA,
                                    verbose = False)
    #compute scores (each doc against each corpus)
    df = model.internal_stats(LOO = True) #use LOO = true only for rank-based testing
    df = df.dropna()
    dfi = df.filter(['doc_id', 'author', 'wrt_author',
                    'HC', 'log-likelihood', 'HC_rank',
                    'chisq', 'chisq_rank', 
                      'log-likelihood_rank'
                    ])
    dfi.loc[:, 'itr'] = i
    res = res.append(dfi, ignore_index = False)
  res.loc[:,'HC'] = res.HC.astype(float)

  filename = './drive/My Drive/Data/biblical_bootstrap_w{}_nBS{}_ng{}.csv'.format(vocab_size, nBS, ng_range)
  res.to_csv(filename)

res = pd.read_csv('./drive/My Drive/Data/biblical_bootstrap_w500_nBS100.csv')
N_WORDS = 1000

"""### Bootstrap over Verses"""

no_verse = len(ds.verse.unique())

for vocab_size in [100] :
  most_freq = n_most_frequent_by_author(
      ds[ds.author.isin(['Dtr', 'DtrH', 'P'])],
                                      vocab_size)

  res = pd.DataFrame()
  for i in range(nBS) :
    # optional: one bootstrap iteration:

    ver_smp = np.random.choice(ds.verse.unique(), size = no_verse, replace=True)
    dsBS = ds.set_index('verse').loc[ver_smp.tolist(), :].reset_index()
    
    model = AuthorshipAttributionDTM(dsBS, 
                                    vocab = most_freq,
                                    min_cnt = min_cnt,
                                        alpha = alpha,
                                        pval_thresh=pval_thresh,
                    randomize = False, verbose = False)
    #compute scores (each doc against each corpus)
    df = model.internal_stats(LOO = True) #use LOO = true only for rank-based testing
    df = df.dropna()
    dfi = df.filter(['doc_id', 'author', 'wrt_author',
                    'HC', 'log-likelihood', 'HC_rank',
                    'chisq', 'chisq_rank', 
                      'log-likelihood_rank'
                    ])
    dfi.loc[:, 'itr'] = i
    res = res.append(dfi, ignore_index = False)
  res.loc[:,'HC'] = res.HC.astype(float)

  res.to_csv('./drive/My Drive/Data/biblical_bootstrap_verse_w{}_nBS{}.csv'.format(vocab_size, nBS))

res = pd.read_csv('./drive/My Drive/Data/biblical_bootstrap_verse_w100_nBS500.csv')
N_WORDS = 100

"""### Bootstrapped probabilities analysis"""

wrt_authors = ['Dtr', 'DtrH', 'P']
df1 = res[res.wrt_author.isin(wrt_authors) & res.author.str.contains('UNK')].reset_index()

df1[df1.HC_rank < 1].doc_id.unique()

# compute prob of success for each labeled doc:

def get_association(df, value, wrt_authors) : 
  df1 = df[df.wrt_author.isin(wrt_authors)].reset_index()

  idx_min = df1.groupby(['itr', 'doc_id', 'author'])[value].idxmin()

  res_min = df1.loc[idx_min, :]

  for auth in wrt_authors :
     res_min.loc[:, auth] = (res_min.wrt_author == auth) + 0.
  return res_min

known_authors = ['P', 'DtrH', 'Dtr']
res_min = get_association(res, value = 'HC', wrt_authors = known_authors)


res_av = res_min.groupby(['doc_id', 'author']).mean().reset_index()\
      .melt(id_vars=['doc_id', 'author'], value_vars=['P','DtrH','Dtr'])\
      .rename(columns={'variable' : 'prob', 'author' : 'true author'}, )

plotnine.options.figure_size = (7.4, 5)

p = (ggplot(aes(x='doc_id', y='value', fill = 'prob'),
            data = res_av[~res_av['true author'].isin(known_authors)]) 
 + geom_bar(stat='identity') + coord_flip() + ggtitle('Bootstrapped probs (UNK docs)'))
p.save('./drive/My Drive/Figs/prob_UNK_w{}_verse.png'.format(N_WORDS))
print(p)

plotnine.options.figure_size = (7.4, 14)

p = (ggplot(aes(x='doc_id', y='value', fill = 'prob', color='true author'),
            data = res_av[res_av['true author'].isin(known_authors)]) 
 + geom_bar(stat='identity') + coord_flip() + ggtitle('Bootstrapped probs (known docs)'))
p.save('./drive/My Drive/Figs/prob_known_w{}_verse.png'.format(N_WORDS))
print(p)

"""## Simulate Null Distribution
1. Sample a contigous part from each corpus 
2. Compute HC similarity score of this document against the rest of the corpus
3. Use the historgram as the null distributon
4. Compute P-value of other document with respect to this null
5. Length of sampled part equals to the length of tested document, hence repeat the process for each document length
"""

def test_doc_corpus(df_corpus, df_doc, nMonte,
                     sampling_method = 'lemma', contig = True) :
  """ 
  Args:
  -----
  df_corpus : corpus 
  df_doc : tested corpus
  nMonte : number of samples
  sampling_method : only support 'lemma' at the moment
  contig : indicate wether to sample contigous chunks

  Returns:
  -------
  res : pd df summarizing result of each iteration

  it is assumed that df_doc is not part of df_corpus
  """
  df_corpus.loc[:,'type'] = 'corpus'
  df_doc.loc[:,'type'] = 'doc'

  ds1 = df_corpus.append(df_doc)
  ds1 = ds1.drop('author', axis=1).rename(columns={'type' : 'author'})
  ds1 = ds1.reset_index()

  # get HC score between doc and corpus
  md = AuthorshipAttributionDTM(ds1,
        vocab = most_freq,
        min_cnt = MIN_CNT,
          alpha = ALPHA, 
          verbose = False)

  hc0 = md.two_author_test('corpus','doc').HC.values[0]

  # sample from combined corpus

  if sampling_method == 'lemma' :
    ln = len(ds1[ds1['author'] == 'doc'])
    pool = ds1.index.tolist()
  
  if sampling_method == 'verse' :
    ln = len(ds1[ds1['author'] == 'doc'].verse.unique())
    pool = ds1.verse.unique()

  res = pd.DataFrame()
  for i in range(nMonte) :
    if contig :
      k = np.random.randint(1, len(pool)-ln) #sample contigous part
      smp = pool[k:k+ln]
    else :
      smp = np.random.choice(pool, size = ln, replace=False)
    
    if sampling_method == 'lemma' :
      ds1.loc[:, 'author'] = 'corpus'
      ds1.loc[smp, 'author'] = 'doc'

    elif sampling_method == 'verse' :
      ds1.loc[:, 'author'] = 'corpus'
      ds1.set_index('verse').loc[smp.tolist(), 'author'] = 'doc'

    md = AuthorshipAttributionDTM(ds1, min_cnt = MIN_CNT,
                                  vocab=most_freq,
            alpha = ALPHA, verbose = False)
    
    hc1 = md.two_author_test('doc','corpus').HC.values[0]
    res = res.append({'itr' : i, 'HC' : hc1, 'len' : ln,
                      'alpha' : ALPHA, 'min_cnt' : MIN_CNT,
                      'sampling_method' : sampling_method, 'contig' : contig
                      },
                ignore_index=True)

  res.loc[:,'HC0'] = hc0
  return res

lo_docs = ds.doc_id.unique()
#lo_docs = new_dc_list

nMonte = 500
res = pd.DataFrame()
filename = './drive/My Drive/Data/simulated_null_nMonte{}_ng{}.csv'.format(nMonte,ng_range)

for dc in lo_docs :
  for auth in lo_known_authors :
    df_corpus = ds[(ds.author == auth) & (ds.doc_id != dc)]
    df_doc = ds[ds.doc_id == dc]
    res1 = test_doc_corpus(df_corpus, df_doc, nMonte = nMonte, contig=True)
    res1.loc[:,'doc_id'] = dc
    res1.loc[:,'author'] = df_doc.author.values[0]
    res1.loc[:,'wrt_author'] = auth
    res = res.append(res1)
  res.to_csv(filename)

#! ls 'drive/My Drive/Data/'

filename = 'drive/My Drive/Data/simulated_null_nMonte500_ng(2, 2).csv'
res = pd.read_csv(filename)

res[(res.doc_id=='2Sam.7') & (res.author == 'DtrH')].HC.hist()

res[res.doc_id.str.contains('UNK3') & (res.wrt_author == 'Dtr')].HC.hist()

res[(res.author=='P') & (res.wrt_author == 'P')].HC.hist()

lo_known_authors = ['Dtr', 'DtrH', 'P']
dft = pd.DataFrame()
for r in res.groupby(['doc_id', 'wrt_author', 'author']) :
  pval = np.mean(r[1]['HC0'] <= r[1]['HC'])
  dft = dft.append({'doc_id' : r[0][0], 'wrt_author' : r[0][1],
                    'author' : r[0][2],
                    'pval' : pval,
                    }, ignore_index = True)

dft_known = dft[dft.author.isin(lo_known_authors)]
idx_min = dft_known.groupby(['doc_id', 'author'])['pval'].idxmax()
res_min = dft_known.loc[idx_min, :]
res_min.loc[:, 'succ'] = res_min.author == res_min.wrt_author
print(res_min.succ.mean())

"""For knonw documents, can be reject the null hypothesis at 0.05 for all author ?"""

np.mean(dft_known[dft_known.pval > 0.05/3].groupby('doc_id').pval.count() == 0)

"""No!"""

res[res.doc_id == 'Exod.35'].groupby(['wrt_author']).HC.hist()

from matplotlib import pyplot as plt
res[res.doc_id == '1Kgs.8'].groupby(['wrt_author']).HC.hist()

seed = 100
nBS = 100 #@param {type:"slider", min:0, max:2000, step:100}

res0 = pd.read_csv(filename)

"""to do: show HC scores of tested docs over null plot"""

p = (ggplot(aes(x='HC',fill = 'author'), data = res) + geom_histogram() + facet_wrap('tested_doc'))
print(p)

"""### Test against null"""

min_cnt = 10
model = AuthorshipAttributionDTM(ds, 
                              min_cnt = MIN_CNT,
                              alpha=ALPHA, 
                              verbose=False)

df = model.internal_stats(wrt_authors=['Dtr', 'DtrH', 'P'])

dft = df
lo_unks = ds[ds.author.str.contains('UNK')].author.unique()
#dft = df[df.author.str.contains('UNK')]

def pval(x, wrt_author, length) :
  dfa = res0[(res0.author == wrt_author) & (res0['len'] == length)]
  N = len(dfa)
  return 1 - np.sum(dfa.HC < x) / (N + 1)

dfr = pd.DataFrame()
for r in dft.iterrows() :
  if r[1].author in lo_unks :
    ln = len(ds[ds.author == r[1]['author']])
  else :
    ln = len(ds[ds.author == 'UNK1:Ark'])
  dfr = dfr.append({'index' : r[0], 'doc_id' : r[1]['doc_id'],
                    'author' : r[1]['author'],
                    'wrt_author' : r[1]['wrt_author'], 'HC' : r[1]['HC'],
                    'pval' : pval(r[1]['HC'], r[1]['wrt_author'],
                                  ln)},
                   ignore_index = True)

def prob_of_succ(df, value, known_authors, plot=False) :
  # compute prob of success for each labeled doc:
  df.loc[:,value] = df[value].astype(float)
  res1 = df[df.author.isin(known_authors) & df.wrt_author.isin(known_authors)].reset_index()
  idx_min = res1.groupby(['doc_id', 'author'])[value].idxmin()
  res_min = res1.loc[idx_min, :]
  res_min.loc[:, 'succ'] = res_min.author == res_min.wrt_author

  succ_per_doc = res_min.groupby('doc_id').succ.mean()
  print("{} prob. of success = {}".format(value,succ_per_doc.mean()) )
  if plot :
    p = ggplot(aes(x='doc_id', fill = 'succ'), data = res_min) + geom_bar(stat='count') + coord_flip()
    print(p)

dfr.loc[:,'1pval'] = 1 - dfr.pval

prob_of_succ(dfr[dfr.author.isin(known_authors)], '1pval', known_authors)

#for dc in lo_unks :
for dc in lo_unks :
  res1 = res0[res0.doc_id == dc]
  res1.loc[:,'type'] = 'null'
  dft = dfr[dfr.author == dc].drop('author', axis=1).rename(columns = {'wrt_author' : 'author'})
  res1.loc[:,'type'] = 'null'
  p =(ggplot(aes(x='HC', fill='author', color='author'), data = res1) + geom_histogram() 
  + geom_vline(aes(xintercept='HC', colour='author'), dft, size=1.5,linetype='dashed')
  #+ geom_vline(xintercept = dft[dft.author == auth].HC, linetype = 'dashed', size=1, color='red')
  + ggtitle(dc)
  )
  p.save('./drive/My Drive/Figs/simulated_null_test_{}.png'.format(dc))
  print(p)

dft[dft.author == auth].HC

seed = 100

nBS = 100 #@param {type:"slider", min:0, max:1000, step:100}
min_cnt = 10 #@param {type:"slider", min:3, max:30, step:1}
alpha = 0.3 #@param {type:"slider", min:0.05, max:0.5, step:0.05}
pval_thresh= 1 #@param {type:"slider", min:0.1, max:1.05, step:0.01}

res = pd.DataFrame()

for i in range(nBS) :
  # optional: one bootstrap iteration:
  dsBS = ds.sample(len(ds), replace=True, random_state=seed+i)

  model = AuthorshipAttributionDTM(dsBS, min_cnt = min_cnt,
                                        alpha = alpha,
                                        pval_thresh=pval_thresh,
                  randomize = False, verbose = False)
  #compute scores (each doc against each corpus)
  df = model.internal_stats(LOO = True) #use LOO = true only for rank-based testing
  df = df.dropna()
  dfi = df.filter(['doc_id', 'author', 'wrt_author',
                   'HC', 'log-likelihood', 'HC_rank',
                   'chisq', 'chisq_rank', 
                    'log-likelihood_rank'
                   ])
  dfi.loc[:, 'itr'] = i
  res = res.append(dfi, ignore_index = False)
res.loc[:,'HC'] = res.HC.astype(float)
res.to_csv('./drive/My Drive/Data/biblical_bootstrap.csv')

"""## Identifying discreminating words"""

def process_morphology(ds) :
    #remove proper names:
    
    def remove_POS(df, code) :
        return df[~df.morph.str.containt(code)]
    
    def replace_POS(df, code, value) :
        df.loc[df.morph.str.contains(code), 'lemma'] = value
        return df
    
    def keep_only_POS(df, code)  :
        return df[df.morph.str.contains(code)]
    
    ds = replace_POS(ds, 'Np', 'PROPN') #remove proper names
    #ds = replace_POS(ds, 'Nc', 'COMN') # remove common nouns
    #ds = replace_POS(ds, 'Ng', 'GENN') # remove gentilic nouns    
    #ds = replace_POS(ds, 'Ac', 'CD') # remove cardinal digits
    #ds = replace_POS(ds, 'V', 'VERB') # remove verbs
    
    return ds

ds = data.copy()
ds = process_morphology(ds)
ds = ds.filter(['chapter', 'lemma', 'author', 'morph'])\
          .rename(columns = {'chapter' : 'doc_id', 'lemma' : 'term'})

#ds = explode_str(ds, 'term', '/')

lo_authors = ['DtrH', 'Dtr', 'P']

feat = []
for auth in lo_authors :
    ds1 = ds[ds.author.isin(lo_authors)]
    ds1.loc[ds1.author != auth, 'author'] = 'others'
    model =  AuthorshipAttributionDTM(ds1, min_cnt = 10, randomize = False, alpha=0.3)
    df1 = model.two_author_test(auth, 'others', stbl=False)
    df1.loc[:,'term'] = df1.feat.apply(lemma_to_term)
    feat1 = df1[df1.thresh].feat.tolist()
    feat += feat1
    print("{}: found {} features".format(auth,len(feat1)))

feat = list(set(feat))
#df1[df1.thresh].sort_values('pval')

model_red = AuthorshipAttributionDTM(ds, 
                                      vocab = feat,
                                      alpha=0.15)
df = model_red.internal_stats(LOO = True)
df = df.dropna()

authors = ['DtrH', 'Dtr', 'P']
prob_of_succ(df, value = 'HC', known_authors = authors)
prob_of_succ(df, value = 'HC_rank', known_authors = authors)

prob_of_succ(df, value = 'log-likelihood', known_authors = authors)
prob_of_succ(df, value = 'log-likelihood_rank', known_authors = authors)

prob_of_succ(df, value = 'chisq', known_authors = authors)
prob_of_succ(df, value = 'chisq_rank', known_authors = authors)

"""## Bootstrapping with reduced features:
To do: 
Evaluate perfromance of procedure using cross-validation:
1. split tranining/val
2. For each split :
  - find discriminating features
  - compute reduced-feature model
  - evaluate probability of success of validation set
"""

def red_features(ds, feat) :
  return ds[ds.term.isin(feat)]

ds = red_features(ds, feat)

nBS = 100
seed = 100

res = pd.DataFrame()

for i in range(nBS) :
  # optional: one bootstrap iteration:
  dsBS = ds.sample(len(ds), replace=True, random_state=seed+i)

  model = AuthorshipAttributionDTM(dsBS, min_cnt = 10, randomize = False, verbose = False)
  #compute scores (each doc against each corpus)
  df = model.internal_stats(LOO = True) #use LOO = true only for rank-based testing
  df = df.dropna()
  dfi = df.filter(['doc_id', 'author', 'wrt_author', 'HC', 'log-likelihood', 'HC_rank', 'chisq',
                   'log-likelihood_rank', 'chisq_rank'
                   ])
  dfi.loc[:, 'itr'] = i
  res = res.append(dfi, ignore_index = False)
res.loc[:,'HC'] = res.HC.astype(float)

res.to_csv('./drive/My Drive/Data/biblical_bootstrap_red_feat.csv')

res = pd.read_csv('./drive/My Drive/Data/biblical_bootstrap_red_feat.csv')

# res.to_csv('./drive/My Drive/Data/biblical_bootstrap.csv')
#res.groupby(['doc_id', 'author', 'wrt_author']).mean().reset_index()

known_authors = ['P', 'DtrH', 'Dtr']
res_min = get_association(res, value = 'HC_rank', wrt_authors = known_authors)

res_av = res_min.groupby(['doc_id', 'author']).mean().reset_index()\
      .melt(id_vars=['doc_id', 'author'], value_vars=['P','DtrH','Dtr'])\
      .rename(columns={'variable' : 'prob', 'author' : 'true author'}, )

plotnine.options.figure_size = (7.4, 5)

p = (ggplot(aes(x='doc_id', y='value', fill = 'prob'),
            data = res_av[~res_av['true author'].isin(known_authors)]) 
 + geom_bar(stat='identity') + coord_flip() + ggtitle('Bootstrapped probs (UNK docs)'))
p.save('./drive/My Drive/prob_UNK.png')
print(p)

plotnine.options.figure_size = (7.4, 14)

p = (ggplot(aes(x='doc_id', y='value', fill = 'prob', color='true author'),
            data = res_av[res_av['true author'].isin(known_authors)]) 
 + geom_bar(stat='identity') + coord_flip() + ggtitle('Bootstrapped probs (known docs)'))
p.save('./drive/My Drive/prob_known.png')
print(p)

[lemma_to_term(f) for f in feat]



"""# Features HC finds as discriminating between 'Dtr' and 'DtrH':"""

df1.loc[:,'morph'] = df1.feat.apply(lemma_to_morph)
df1[df1.thresh].sort_values('pval')

f